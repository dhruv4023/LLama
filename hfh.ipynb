{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.llamaindex.ai/en/stable/examples/llm/huggingface/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"transformers[torch]\" \"huggingface_hub[inference]\"\n",
    "# %pip install llama-index-llms-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "u:\\Files\\LamaIndex\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "model-00008-of-00008.safetensors: 100%|██████████| 816M/816M [02:26<00:00, 5.57MB/s]\n",
      "Downloading shards: 100%|██████████| 8/8 [02:28<00:00, 18.56s/it]\n",
      "u:\\Files\\LamaIndex\\.venv\\lib\\site-packages\\accelerate\\utils\\modeling.py:1363: UserWarning: Current model requires 1040195328 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:05<00:00,  1.53it/s]\n",
      "generation_config.json: 100%|██████████| 111/111 [00:00<00:00, 28.7kB/s]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:llama_index.llms.huggingface.base:The model `HuggingFaceH4/zephyr-7b-alpha` and tokenizer `StabilityAI/stablelm-tuned-alpha-3b` are different, please ensure that they are compatible.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_index.llms.huggingface import (\n",
    "    HuggingFaceLLM,\n",
    ")\n",
    "\n",
    "# This uses https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha\n",
    "# downloaded (if first invocation) to the local Hugging Face model cache,\n",
    "# and actually runs the model on your local machine's hardware\n",
    "locally_run = HuggingFaceLLM(model_name=\"HuggingFaceH4/zephyr-7b-alpha\")\n",
    "\n",
    "# This will use the same model, but run remotely on Hugging Face's servers,\n",
    "# accessed via the Hugging Face Inference API\n",
    "# Note that using your token will not charge you money,\n",
    "# # the Inference API is free it just has rate limits\n",
    "# remotely_run = HuggingFaceInferenceAPI(\n",
    "#     model_name=\"HuggingFaceH4/zephyr-7b-alpha\", token=HF_TOKEN\n",
    "# )\n",
    "\n",
    "# # Or you can skip providing a token, using Hugging Face Inference API anonymously\n",
    "# remotely_run_anon = HuggingFaceInferenceAPI(\n",
    "#     model_name=\"HuggingFaceH4/zephyr-7b-alpha\"\n",
    "# )\n",
    "\n",
    "# # If you don't provide a model_name to the HuggingFaceInferenceAPI,\n",
    "# # Hugging Face's recommended model gets used (thanks to huggingface_hub)\n",
    "# remotely_run_recommended = HuggingFaceInferenceAPI(token=HF_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
