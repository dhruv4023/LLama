{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-vector-stores-faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "u:\\Files\\LamaIndex\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\Dhruv Patel\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\Dhruv Patel\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "local model embedding\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-small-en\n",
      "Load pretrained SentenceTransformer: BAAI/bge-small-en\n",
      "Load pretrained SentenceTransformer: BAAI/bge-small-en\n"
     ]
    }
   ],
   "source": [
    "# MODEL_REPO = \"thenlper/gte-small\"\n",
    "TEXT_GENERATION_MODEL_REPO = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# TEXT_GENERATION_MODEL_REPO = \"HuggingFaceH4/zephyr-7b-alpha\"\n",
    "# MODEL_REPO = \"Writer/camel-5b-hf\"\n",
    "from my_settings import LLMSettings\n",
    "\n",
    "model = LLMSettings(\n",
    "    model_name=TEXT_GENERATION_MODEL_REPO,\n",
    "    context_window=10000,\n",
    "    max_new_tokens=10000,\n",
    "    max_length=10000,\n",
    "    temperature=0.85,\n",
    "    query_wrapper_prompt=\"\"\"\n",
    "    Below is an instruction that describes a task. \n",
    "    You'r helpful AI assisant given the task to help people seeking advise.\n",
    "    Use the following pieces of context to answer the question at the end.\n",
    "    The text provided to you is about indian constitution and legal processes and provide information in a lawful manner\n",
    "    You should prefer information which are more related to asked question.\n",
    "    Make sure to rely on information from text only and not on questions to provide accurate responses.\n",
    "    When you find particular answer in the context useful, make sure to cite it in the your answer.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Each answer you generate should contain a section numbers you found useful.\n",
    "    You can only use the given to you to answer the question.\n",
    "    Generate concise answers and relevant data related to the asked question.\n",
    "    You must represent the answer in proper format such as make points highlight some major information.\n",
    "    don't attach your created quetions. if you don't get answer from the given text just say i don't know and terminate answering.\n",
    "    if you get answer from the text than write all about the asked quetion and relevant data related to it.\n",
    "    don't use your own knowledge just use the provided text to answer the question.  \n",
    "    also don't provide such example quetions with answer. just give the answer asked by the user in proper manner.\n",
    "    ### Instruction:\\n{query_str}\\n\\n### Response:\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "model._set_huggingface_embedding(local=True)\n",
    "model._setup_huggingface_llm_inference_api()\n",
    "model._configure_mongodb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = SimpleDirectoryReader(input_files=[\"./data/IPC_186045.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x000001CAD3F9FE70> >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# dimensions of \"BAAI/bge-small-en\"\n",
    "d = 384\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "faiss_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.56s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.14s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.35s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.88s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.16s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.15s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.68s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.60s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.86s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.62s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.46s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.20s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.99s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.93s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.49s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.29s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:11<00:00, 11.44s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.69s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:08<00:00,  8.58s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.87s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.53s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.96s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.66s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.67s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.03s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.46s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.63s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.17s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.22s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.06s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.33s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.97s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.11s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.30s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.03s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.74s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index to disk\n",
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading llama_index.vector_stores.faiss.base from ./storage\\default__vector_store.json.\n",
      "Loading llama_index.vector_stores.faiss.base from ./storage\\default__vector_store.json.\n",
      "Loading llama_index.vector_stores.faiss.base from ./storage\\default__vector_store.json.\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core import (\n",
    "    load_index_from_storage,\n",
    "    StorageContext,\n",
    ")\n",
    "# load index from disk\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"./storage\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"./storage\"\n",
    ")\n",
    "index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = index.as_retriever(similarity_top_k=3)\n",
    "# nodes = retriever.retrieve(\"flower\")\n",
    "\n",
    "# for node in nodes:\n",
    "#     print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    kwargs={\"early_stopping\": True, \"min_length\": 2000, \"max_tokens\": 5000}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.22it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Calculated available context size -76 was not non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Markdown, display\n\u001b[1;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtell me about rioting !\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m display(Markdown(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32md:\\Files\\LamaIndex\\.venv\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32md:\\Files\\LamaIndex\\.venv\\lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py:53\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     52\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 53\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m dispatch_event(QueryEndEvent())\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[1;32md:\\Files\\LamaIndex\\.venv\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32md:\\Files\\LamaIndex\\.venv\\lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:190\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    187\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[0;32m    188\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[0;32m    189\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[1;32m--> 190\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_response_synthesizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_bundle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\Files\\LamaIndex\\.venv\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32md:\\Files\\LamaIndex\\.venv\\lib\\site-packages\\llama_index\\core\\response_synthesizers\\base.py:241\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[1;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m     query \u001b[38;5;241m=\u001b[39m QueryBundle(query_str\u001b[38;5;241m=\u001b[39mquery)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    238\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE,\n\u001b[0;32m    239\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query\u001b[38;5;241m.\u001b[39mquery_str},\n\u001b[0;32m    240\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[1;32m--> 241\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[0;32m    242\u001b[0m         query_str\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39mquery_str,\n\u001b[0;32m    243\u001b[0m         text_chunks\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    244\u001b[0m             n\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mLLM) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[0;32m    245\u001b[0m         ],\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m    247\u001b[0m     )\n\u001b[0;32m    249\u001b[0m     additional_source_nodes \u001b[38;5;241m=\u001b[39m additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    250\u001b[0m     source_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nodes) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[1;32md:\\Files\\LamaIndex\\.venv\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32md:\\Files\\LamaIndex\\.venv\\lib\\site-packages\\llama_index\\core\\response_synthesizers\\compact_and_refine.py:42\u001b[0m, in \u001b[0;36mCompactAndRefine.get_response\u001b[1;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get compact response.\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m new_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_compact_text_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[0;32m     44\u001b[0m     query_str\u001b[38;5;241m=\u001b[39mquery_str,\n\u001b[0;32m     45\u001b[0m     text_chunks\u001b[38;5;241m=\u001b[39mnew_texts,\n\u001b[0;32m     46\u001b[0m     prev_response\u001b[38;5;241m=\u001b[39mprev_response,\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m     48\u001b[0m )\n",
      "File \u001b[1;32md:\\Files\\LamaIndex\\.venv\\lib\\site-packages\\llama_index\\core\\response_synthesizers\\compact_and_refine.py:57\u001b[0m, in \u001b[0;36mCompactAndRefine._make_compact_text_chunks\u001b[1;34m(self, query_str, text_chunks)\u001b[0m\n\u001b[0;32m     54\u001b[0m refine_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_refine_template\u001b[38;5;241m.\u001b[39mpartial_format(query_str\u001b[38;5;241m=\u001b[39mquery_str)\n\u001b[0;32m     56\u001b[0m max_prompt \u001b[38;5;241m=\u001b[39m get_biggest_prompt([text_qa_template, refine_template])\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prompt_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Files\\LamaIndex\\.venv\\lib\\site-packages\\llama_index\\core\\indices\\prompt_helper.py:276\u001b[0m, in \u001b[0;36mPromptHelper.repack\u001b[1;34m(self, prompt, text_chunks, padding, llm)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepack\u001b[39m(\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    265\u001b[0m     prompt: BasePromptTemplate,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    268\u001b[0m     llm: Optional[LLM] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    269\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Repack text chunks to fit available context window.\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m    This will combine text chunks into consolidated chunks\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03m    that more fully \"pack\" the prompt template given the context_window.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m     text_splitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_splitter_given_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     combined_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([c\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m text_chunks \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mstrip()])\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text_splitter\u001b[38;5;241m.\u001b[39msplit_text(combined_str)\n",
      "File \u001b[1;32md:\\Files\\LamaIndex\\.venv\\lib\\site-packages\\llama_index\\core\\indices\\prompt_helper.py:234\u001b[0m, in \u001b[0;36mPromptHelper.get_text_splitter_given_prompt\u001b[1;34m(self, prompt, num_chunks, padding, llm)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_text_splitter_given_prompt\u001b[39m(\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    226\u001b[0m     prompt: BasePromptTemplate,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m     llm: Optional[LLM] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TokenTextSplitter:\n\u001b[0;32m    231\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get text splitter configured to maximally pack available context window,\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m    taking into account of given prompt, and desired number of chunks.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_available_chunk_size\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not positive.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Files\\LamaIndex\\.venv\\lib\\site-packages\\llama_index\\core\\indices\\prompt_helper.py:218\u001b[0m, in \u001b[0;36mPromptHelper._get_available_chunk_size\u001b[1;34m(self, prompt, num_chunks, padding, llm)\u001b[0m\n\u001b[0;32m    215\u001b[0m     prompt_str \u001b[38;5;241m=\u001b[39m get_empty_prompt_txt(prompt)\n\u001b[0;32m    216\u001b[0m     num_prompt_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_counter\u001b[38;5;241m.\u001b[39mget_string_tokens(prompt_str)\n\u001b[1;32m--> 218\u001b[0m available_context_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_available_context_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_prompt_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m result \u001b[38;5;241m=\u001b[39m available_context_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_chunks \u001b[38;5;241m-\u001b[39m padding\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Files\\LamaIndex\\.venv\\lib\\site-packages\\llama_index\\core\\indices\\prompt_helper.py:150\u001b[0m, in \u001b[0;36mPromptHelper._get_available_context_size\u001b[1;34m(self, num_prompt_tokens)\u001b[0m\n\u001b[0;32m    148\u001b[0m context_size_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_window \u001b[38;5;241m-\u001b[39m num_prompt_tokens \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_output\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context_size_tokens \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculated available context size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_size_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not non-negative.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m     )\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m context_size_tokens\n",
      "\u001b[1;31mValueError\u001b[0m: Calculated available context size -76 was not non-negative."
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "response = query_engine.query(\"tell me about rioting !\")\n",
    "display(Markdown(f\"{response}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
